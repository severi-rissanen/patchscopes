{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patchscopes Demo - Reproducing Figure 1\n",
        "\n",
        "This notebook demonstrates the core patchscoping idea from the paper:\n",
        "\n",
        "1. **Extract** a representation of a token from a source prompt (e.g., \"CEO\")\n",
        "2. **Patch** it into an identity-style target prompt\n",
        "3. **Decode** what the model thinks that token means by looking at top-k predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import torch\n",
        "from patchscopes.model import load_model\n",
        "from patchscopes.prompts import build_identity_prompt\n",
        "from patchscopes.positions import find_substring_token_position, find_token_position, verify_single_token\n",
        "from patchscopes.patch import extract_representation, run_with_patch, get_top_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "\n",
        "# Patchscope settings\n",
        "SOURCE_TEXT = \"Amazon 's former CEO attended Oscars\"\n",
        "TARGET_WORD = \"CEO\"\n",
        "LAYERS_TO_TEST = [0, 5, 10, 15, 20, 25, 31]\n",
        "TOP_K = 5\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = load_model(MODEL_NAME, device=DEVICE, dtype=DTYPE)\n",
        "print(f\"Loaded {MODEL_NAME} with {model.cfg.n_layers} layers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find target word position in source text\n",
        "target_position = find_substring_token_position(model, SOURCE_TEXT, TARGET_WORD)\n",
        "assert target_position is not None, f\"Could not find '{TARGET_WORD}' in source text\"\n",
        "\n",
        "# Show tokenization\n",
        "tokens = model.to_tokens(SOURCE_TEXT, prepend_bos=True)\n",
        "print(f\"Source: '{SOURCE_TEXT}'\\n\")\n",
        "print(\"Tokenization:\")\n",
        "for i in range(tokens.shape[1]):\n",
        "    marker = \" <-- TARGET\" if i == target_position else \"\"\n",
        "    print(f\"  {i}: '{model.to_string(tokens[0, i])}'{marker}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build identity-style target prompt\n",
        "placeholder = \" x\"\n",
        "if not verify_single_token(model, placeholder):\n",
        "    placeholder = \" X\"\n",
        "    assert verify_single_token(model, placeholder), \"Could not find single-token placeholder\"\n",
        "\n",
        "target_prompt = build_identity_prompt(placeholder=placeholder, num_demos=5)\n",
        "placeholder_pos = find_token_position(model, target_prompt, placeholder)\n",
        "\n",
        "print(f\"Target prompt: '{target_prompt}'\")\n",
        "print(f\"Placeholder position: {placeholder_pos}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run patchscope across layers (extract from each layer, patch at layer 0)\n",
        "layers = [l for l in LAYERS_TO_TEST if l < model.cfg.n_layers]\n",
        "\n",
        "for layer in layers:\n",
        "    # Extract representation from source at specified layer\n",
        "    source_vec = extract_representation(model, SOURCE_TEXT, layer=layer, position=target_position)\n",
        "    \n",
        "    # Patch at layer 0 and get logits\n",
        "    logits = run_with_patch(model, target_prompt, target_position=placeholder_pos, source_vec=source_vec)\n",
        "    \n",
        "    # Get top-k predictions\n",
        "    prediction_pos = min(placeholder_pos + 1, logits.shape[1] - 1)\n",
        "    top_tokens = get_top_tokens(model, logits, prediction_pos, k=TOP_K)\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"Layer {layer:2d}: \", end=\"\")\n",
        "    print(\" | \".join([f\"'{tok}' ({prob:.3f})\" for tok, prob in top_tokens]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expected Behavior\n",
        "\n",
        "- **Early layers (0-5):** Predictions may be incoherent or generic tokens\n",
        "- **Middle/late layers (15+):** Predictions should relate to the meaning of \"CEO\" (e.g., \"Jeff\", \"Bezos\", \"executive\", \"chief\", etc.)\n",
        "\n",
        "This demonstrates how the model's representation of \"CEO\" evolves through layers, from surface-level features to semantic understanding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
